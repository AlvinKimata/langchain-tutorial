{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e21ebb-90b9-442e-8e69-1eea1642fef3",
   "metadata": {},
   "source": [
    "Agents can be unreliable and may need human input to successfully accomplish tasks. Similarly, for some actions, you may want to require human approval before running to ensure that everything is running as intended.\n",
    "\n",
    "LangGraph supports `human-in-the-loop` workflows in a number of ways. In this section, we will use LangGraph's `interrupt_before` functionality to always break the tool node.\n",
    "\n",
    "First, start from our existing code. The following is copied from Part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71e92357-4b23-4032-80c1-7afb880449a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "989a63de-cdee-4318-87ef-3f8868fd8fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter TAVILY_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"04. Human in the loop.\"\n",
    "\n",
    "def _set_env(var: str):\n",
    "  if not os.environ.get(var):\n",
    "    os.environ[var] = getpass(f\"Enter {var}: \")\n",
    "\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "_set_env(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c55ba3a-f49f-4756-9901-ce24e9f777a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "#Create graph builder.\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "tool = TavilySearchResults(max_results=3)\n",
    "tools = [tool]\n",
    "\n",
    "llm = ChatGroq(temperature = 0.1, model = \"Llama3-70b-8192\")\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23369536-602b-41d2-88a6-87df132ff17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools = [tool])\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9c3c24d-0b46-4f57-bc0c-3847755136e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the graph specifying to interrupt_before the action node.\n",
    "graph = graph_builder.compile(\n",
    "    checkpointer = memory,\n",
    "    interrupt_before = [\"tools\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc65543d-c66d-4b32-bf44-19c0cf286ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I'm learning LangGraph. Could you do some research on it for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_bwpx)\n",
      " Call ID: call_bwpx\n",
      "  Args:\n",
      "    query: LangGraph\n"
     ]
    }
   ],
   "source": [
    "user_input = \"I'm learning LangGraph. Could you do some research on it for me?\"\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "#The config is the 2nd positional argument to stream() or invoke()\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", user_input)]}, config, stream_mode = \"values\")\n",
    "\n",
    "\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb41fd1d-4159-4019-81b1-23c420c6e815",
   "metadata": {},
   "source": [
    "***\n",
    "Let's inspect the graph state to confirm it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42834412-c7a3-4608-a0fa-5aa62610da1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tools',)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "snapshot.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c0c8450-c1b1-4ea9-bf8d-316b49598b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'tavily_search_results_json',\n",
       "  'args': {'query': 'LangGraph'},\n",
       "  'id': 'call_bwpx',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_message = snapshot.values[\"messages\"][-1]\n",
    "existing_message.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc744507-69f7-4c61-b5f3-b27723450374",
   "metadata": {},
   "source": [
    "\n",
    "This query seems reasonable. Nothing to filter here. The simplest thing the human can do is just let the graph continue executing. Let's do that below.\n",
    "\n",
    "Next, continue the graph! Passing in `None` will just let the graph continue where it left off, without adding anything new to the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb51c790-33bc-487f-89af-c24f0feafa3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# `None` will append nothing new to the current state, letting it resume as if it had never been interrupted\n",
    "\n",
    "events = graph.stream(None, config, stream_mode = \"values\")\n",
    "\n",
    "for event in events:\n",
    "    if \"messages\" in events:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27ade6-4fd0-49d2-b5f6-8bebdfa313d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
